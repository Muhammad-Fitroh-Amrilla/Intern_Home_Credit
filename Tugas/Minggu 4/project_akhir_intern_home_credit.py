# -*- coding: utf-8 -*-
"""Project Akhir_Intern_Home Credit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tsKWVDb_6gnwyn5pWMLyBAAs2mAScRcA

# Import Library

Mengimport library yang dibutuhkan
"""

# Commented out IPython magic to ensure Python compatibility.
# import library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import average_precision_score

"""# Data Loading

Mengunduh data dari sumber https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/home-credit-indonesia/home-credit-default-risk.zip kemudian mengekstrak file dataset dan dijadikan dataframe. Data yang akan digunakan yaitu application_train.csv
"""

!wget "https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/home-credit-indonesia/home-credit-default-risk.zip"

!unzip home-credit-default-risk.zip

df = pd.read_csv('/content/application_train.csv')
df

"""Output kode diatas memberikan informasi sebagai berikut:

*   Terdapat 307511 baris dalam dataset 
*   Ada 122 Kolom dalam dataset

# Explanatory Data Analysis

Melakukan beberapa tahapan sebagai berikut : 

1.   Deskripsi Variabel
2.   Menangani missing value dan duplikat data
3.   Analisis Univariate : fitur kategorik dan numerik
4.   Analisis Multivariate : fitur kategorik dan numerik

<h2> Dekspripsi Variabel <h2>
"""

# Cek info dataset
df.info()

"""Dari output dapat dilihat bahwa:

*   Ada 16 kolom bertipe object
*   Terdapat 41 kolom dengan tipe data int64
*   Terdapat 65 kolom dengan tipe data float64


"""

# Cek deskripsi data
df.describe()

"""<h2> Menangani missing value <h2>

Melakukan pengecekan terlebih dahulu apakah didalam dataset terdapat missing value dengan kode berikut :
"""

# Fungsi untuk mencari nilai yang kosong (missing value)
def missing_values(df):
    n_miss_val = df.isnull().sum()
    n_miss_per = 100 * df.isnull().sum() / len(df)
    miss_tbl = pd.concat([n_miss_val,n_miss_per],axis=1).sort_values(1,ascending=False).round(1)
    miss_tbl = miss_tbl[miss_tbl[1] !=0]
    
    miss_tbl = miss_tbl.rename(columns ={0: 'Missing Values',1:'%(Percentage) Missing Values'})
    print("{} columns that have missing values.".format(miss_tbl.shape[0]))
    
    return miss_tbl

# Cek missing value di data train
missing_values_table = missing_values(df)
missing_values_table

"""Dapat dilihat bahwa ada 64 kolom yang memiliki missing value, langkah mengatasi missing value yaitu: 
* Drop kolom yang memiliki missing value lebih dari 25 %
* Mengganti missing value pada data numerik dengan nilai mean
* Mengganti missing calue pada data kategorik dengan nilai modus

"""

# cek missing value dengan nilai lebih dari 25%
col_mv = missing_values_table[missing_values_table['%(Percentage) Missing Values'] > 25]
col_mv

# Mengatasi missing value dengan drop kolom yang memiliki missing value lebih dari 25 %
df = df.drop(columns=col_mv.index)

df.shape

# Mengatasi missing value pada data numerik dengan modus
cat_dat = df.select_dtypes(include='object').columns.to_list()
for i in cat_dat:
    df[i].fillna(df[i].mode()[0], inplace=True)

# Mengatasi missing value pada data numerik dengan mean
num_dat = df.select_dtypes(include=np.number).columns.to_list()
for i in num_dat:
  df[i]=df[i].fillna(df[i].mean())

# Cek ulang missing value di data train
missing_values_table = missing_values(df)
missing_values_table

"""Dapat dilihat bahwa missing value pada dataset telah tiada

<h2> Menangani Duplicated Data <h2>
"""

df.duplicated().sum()

"""Dapat dilihat bahwa tidak terdapat duplicated data

<h2> Analisis Univariate <h2>

Analisis univariate merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah.

<h4> Analisis fitur kategorik <h4>
"""

# fitur NAME_CONTRACT_TYPE
count = df['NAME_CONTRACT_TYPE'].value_counts()
percent = 100*df['NAME_CONTRACT_TYPE'].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title='NAME_CONTRACT_TYPE');

# fitur CODE_GENDER
count = df['CODE_GENDER'].value_counts()
percent = 100*df['CODE_GENDER'].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title='CODE_GENDER');

# fitur FLAG_OWN_CAR
count = df['FLAG_OWN_CAR'].value_counts()
percent = 100*df['FLAG_OWN_CAR'].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title='FLAG_OWN_CAR');

"""<h4> Analisis fitur numerik <h4>

"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""<h2> Analisis Multivariate <h2>

Multivariate EDA menunjukkan hubungan antara dua atau lebih variabel pada data.

<h4> Analisis fitur kategorik <h4>
"""

fig , axs = plt.subplots(ncols=1,nrows=11,figsize=(20,50))
index=0
axs = axs.flatten()
for cols in cat_dat:
        g = sns.countplot(x=cols,hue='TARGET',data=df,ax=axs[index],palette="spring")
        index +=1

"""<h4> Analisis fitur numerik <h4>"""

# Data Numerik
def numerical_features(df):
    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
    numerical_tbl = pd.DataFrame(df[numerical_cols].dtypes).rename(columns = {0:'Data Types'})
    print("The dataset contains {} numerical values.(included target value)".format(df[numerical_cols].shape[1]))
    
    return numerical_tbl

numerical_features(df)

"""# Data Preparation

Melakukan data preparation dengan 3 tahapan yaitu :


1.   Encoding fitur kategorik
2.   Standarisasi
3.   Train-Test Split

<h2> Encoding Fitur Categorical
"""

# Label Encoding untuk enkode data kategori yang memiliki 1 atau 2 kategori
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

categorical_cols = df.select_dtypes(include=["object"])
l = LabelEncoder()
for p in categorical_cols:
    df[p]=l.fit_transform(df[p])
df

# Hot Encoder untuk enkode data kategori yang memiliki lebih dari 2 kategori. 
# Menggunakan fungsi get_dummies untuk mengubah data kategori menjadi data dummy
df= pd.get_dummies(df)

print('Train data : ', df.shape)

"""<h2> Standarisasi <h2>

Standarisasi menggunakan teknik StandarScaler dari library Scikitlearn, 

StandardScaler melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi.  StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0. Sekitar 68% dari nilai akan berada di antara -1 dan 1.
"""

from sklearn.preprocessing import StandardScaler

# definisikan standard scaler
scaler = StandardScaler()
# transform data
scaled = scaler.fit_transform(df)

"""<h2> Train-Test Split <h2>

proporsi pembagian data latih dan uji adalah 90:10
"""

from sklearn.model_selection import train_test_split
 
X = df.drop(["TARGET"],axis =1)
y = df["TARGET"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

#cek jumlah sampel
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""# Modelling

Modelling adalah tahapan di mana kita menggunakan algoritma machine learning untuk menjawab problem statement dari tahap business understanding.
Pada tahap ini, kita akan mengembangkan model machine learning dengan tiga algoritma. Kemudian, kita akan mengevaluasi performa masing-masing algoritma dan menentukan algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan kita gunakan, antara lain:

1.   Logistic Regression
2.   Decision Tree
3.   Random Forest

"""

# Fungsi model
def predict(chosed_model,name="Model"):
    mdl = chosed_model
    mdl = mdl.fit(X_train,y_train)
    y_prob = mdl.predict_proba(X_test)[:,1]
    y_pred = mdl.predict(X_test)

    print("Performances with {}".format(name))
    auc_test = round(roc_auc_score(y_test,y_prob),2)
    print("AUC Performance: ", auc_test)
    accuracy_test = round(accuracy_score(y_test, y_pred),2)
    print("Accuracy Performance: ", accuracy_test)
    recall_test = round(recall_score(y_test, y_pred, average='weighted'),2)
    print("Recall Performance: ", recall_test)
    precision_test = round(precision_score(y_test, y_pred, average='weighted', zero_division=1),2)
    print("Precision Performance: ", precision_test)
    f1_score_test = round(f1_score(y_test, y_pred, average='weighted'),2)
    print("f1_score Performance: ", f1_score_test)

# Model Logistic Regression
predict((LogisticRegression(max_iter=200)),'LogisticRegression')

# Model Decision Tree 
predict((DecisionTreeClassifier(criterion='gini')),'DecisionTreeClassifier')

# Model Random Forest
predict((RandomForestClassifier(max_depth=4 , random_state=0)),'RandomForestClassifier')